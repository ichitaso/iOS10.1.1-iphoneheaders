/*
* This header is generated by classdump-dyld 1.0
* on Tuesday, December 27, 2016 at 4:54:52 PM Japan Standard Time
* Operating System: Version 10.1.1 (Build 14B150)
* Image Source: /System/Library/PrivateFrameworks/EmbeddedAcousticRecognition.framework/EmbeddedAcousticRecognition
* classdump-dyld is licensed under GPLv3, Copyright Â© 2013-2016 by Elias Limneos.
*/


@protocol OS_dispatch_queue;
#import <EmbeddedAcousticRecognition/EmbeddedAcousticRecognition-Structs.h>
@class _EARSpeechRecognitionAudioBuffer, _EARFormatter, NSObject, NSString, NSData, NSDictionary, _EARSpeechModelInfo;

@interface _EARSpeechRecognizer : NSObject {

	unique_ptr<quasar::SpeechRecognizer, std::__1::default_delete<quasar::SpeechRecognizer> >* _recognizer;
	_EARSpeechRecognitionAudioBuffer* _currentAudioBuffer;
	_EARFormatter* _formatter;
	NSObject*<OS_dispatch_queue> _formatterQueue;
	NSObject*<OS_dispatch_queue> _recognitionQueue;
	NSString* _configPath;
	BOOL _detectUtterances;
	BOOL _concatenateUtterances;
	NSData* _userProfileData;
	double _maximumRecognitionDuration;
	NSDictionary* _recognitionReplacements;

}

@property (nonatomic,copy) NSData * userProfileData;                            //@synthesize userProfileData=_userProfileData - In the implementation block
@property (nonatomic,readonly) _EARSpeechModelInfo * modelInfo; 
@property (assign,nonatomic) BOOL detectUtterances;                             //@synthesize detectUtterances=_detectUtterances - In the implementation block
@property (assign,nonatomic) BOOL concatenateUtterances;                        //@synthesize concatenateUtterances=_concatenateUtterances - In the implementation block
@property (assign,nonatomic) double maximumRecognitionDuration;                 //@synthesize maximumRecognitionDuration=_maximumRecognitionDuration - In the implementation block
@property (nonatomic,copy) NSDictionary * recognitionReplacements;              //@synthesize recognitionReplacements=_recognitionReplacements - In the implementation block
+(void)initialize;
+(id)minimumSupportedConfigurationVersion;
+(id)maximumSupportedConfigurationVersion;
+(id)rawTokenResultsFromRecognitionResults:(id)arg1 ;
-(id)initWithConfiguration:(id)arg1 ;
-(BOOL)detectUtterances;
-(void)setDetectUtterances:(BOOL)arg1 ;
-(double)maximumRecognitionDuration;
-(void)setMaximumRecognitionDuration:(double)arg1 ;
-(_EARSpeechModelInfo *)modelInfo;
-(id)runRecognitionWithResultStream:(id)arg1 ;
-(id)initWithConfiguration:(id)arg1 overrides:(id)arg2 ;
-(id)initWithConfiguration:(id)arg1 withLanguage:(id)arg2 withSdapiConfig:(id)arg3 ;
-(id)initWithConfiguration:(id)arg1 withGeneralVoc:(id)arg2 withLexiconEnh:(id)arg3 withItnEnh:(id)arg4 ;
-(id)initWithConfiguration:(id)arg1 overrides:(id)arg2 generalVoc:(id)arg3 lexiconEnh:(id)arg4 itnEnh:(id)arg5 ;
-(id)initWithConfiguration:(id)arg1 useQuasarFormatter:(BOOL)arg2 ;
-(void)updateUserProfileData:(id)arg1 ;
-(shared_ptr<quasar::SpeechRequestData>*)requestParametersWithUserProfileData:(id)arg1 task:(id)arg2 samplingRate:(unsigned long long)arg3 resultStream:(shared_ptr<quasar::RecogResultStreamBase>*)arg4 extraLanguageModel:(id)arg5 symbolTableList:(const shared_ptr<quasar::SymbolTableList>*)arg6 ;
-(id)runRecognitionWithResultStream:(id)arg1 language:(id)arg2 task:(id)arg3 samplingRate:(unsigned long long)arg4 ;
-(id)recognitionResultsWithAudioData:(id)arg1 userProfileData:(id)arg2 language:(id)arg3 task:(id)arg4 samplingRate:(unsigned long long)arg5 extraLanguageModel:(id)arg6 ;
-(id)recognitionStatistics;
-(NSData *)userProfileData;
-(void)setUserProfileData:(NSData *)arg1 ;
-(BOOL)concatenateUtterances;
-(void)setConcatenateUtterances:(BOOL)arg1 ;
-(NSDictionary *)recognitionReplacements;
-(void)setRecognitionReplacements:(NSDictionary *)arg1 ;
-(void)cancelRecognition;
@end

